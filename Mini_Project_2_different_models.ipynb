{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "8QYbTnRzh4nC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree, svm, naive_bayes,neighbors\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "My3uxRM1iHiM"
   },
   "outputs": [],
   "source": [
    "### Models\n",
    "\n",
    "# Neural Network\n",
    "# MLP = MLPClassifier(solver='adam',activation = 'relu', alpha=0.001, tol=0.002, max_iter = 200, hidden_layer_sizes = (100,10),random_state = 1,verbose = True)\n",
    "MLP = MLPClassifier(solver='adam',activation = 'logistic', tol=0.015, hidden_layer_sizes = (70,), random_state = 1,verbose = True)\n",
    "\n",
    "# Support Vector Machine\n",
    "LSVC = svm.LinearSVC(dual = False)\n",
    "\n",
    "# Gaussian Naive Bayes \n",
    "GNB = naive_bayes.GaussianNB()\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "MNB = naive_bayes.MultinomialNB()\n",
    "\n",
    "# Logistc Regression\n",
    "LRC = LogisticRegression(multi_class='multinomial')\n",
    "\n",
    "# K-NearestNeighbor\n",
    "KNN = neighbors.KNeighborsClassifier(weights='distance')\n",
    "\n",
    "# Decision Tree\n",
    "DTC = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Randon Forest\n",
    "RDTC = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Adaboost\n",
    "ADA = AdaBoostClassifier(n_estimators=50)\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "GBRT = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "\n",
    "# Bagging Decision Tree\n",
    "BDTC = BaggingClassifier(tree.DecisionTreeClassifier(), max_samples=0.5,max_features=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "8ccE-XTgiNiz"
   },
   "outputs": [],
   "source": [
    "### Data Preprocessing\n",
    "def data_preprocess(train_file, test_data):\n",
    "    # load\n",
    "    data_train = np.array(pd.read_csv(train_file))\n",
    "    data_test = np.array(pd.read_csv(test_data))\n",
    "    X_train = data_train[:,0]\n",
    "    y_train = data_train[:,1]\n",
    "    X_test = data_test[:,1]\n",
    "\n",
    "    # Vectorizer and Normalizer\n",
    "    vectorizer = text.TfidfVectorizer(max_features = 20000, binary = False, stop_words = text.ENGLISH_STOP_WORDS)\n",
    "    normalizer_train = preprocessing.Normalizer()\n",
    "    \n",
    "    vectors_train = vectorizer.fit_transform(X_train)\n",
    "    vectors_test = vectorizer.transform(X_test)\n",
    "    vectors_train = normalizer_train.transform(vectors_train).A\n",
    "    vectors_test = normalizer_train.transform(vectors_test).A\n",
    "\n",
    "    return vectors_train, y_train, vectors_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "KPGCruzkiOPe"
   },
   "outputs": [],
   "source": [
    "def main(model, train_file, test_data, numFolds = 10):\n",
    "    start = time.time()\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    # data preprocessing\n",
    "    X_train, y_train, X_test = data_preprocess(train_file, test_data)\n",
    "\n",
    "    # K-fold Cross Validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=numFolds, scoring='accuracy')\n",
    "    Average_accuracy = scores.mean()\n",
    "    print('\\nThe average accuracy of {}-fold cross validation is: {:.5f}'.format(numFolds, Average_accuracy))\n",
    "\n",
    "    # train with the whole training dataset\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print('\\nPerformance metrics:\\n', metrics.classification_report(y_train, y_train_pred))\n",
    "\n",
    "    # predict with test dataset\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print('\\nPredicted labels of the test dataset:\\n', y_test_pred)\n",
    "    \n",
    "    # Run time\n",
    "    end = time.time()\n",
    "    run_time = end - start\n",
    "    print('\\nRun time: {:.2f}s\\n'.format(run_time))\n",
    "\n",
    "    return y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAf1Q5iYiRuK",
    "outputId": "b3ab633a-9760-4c8b-804d-fc852d7ef8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
      "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(70,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=1, shuffle=True, solver='adam',\n",
      "              tol=0.015, validation_fraction=0.1, verbose=True,\n",
      "              warm_start=False)\n",
      "Iteration 1, loss = 1.99026767\n",
      "Iteration 2, loss = 1.87940739\n",
      "Iteration 3, loss = 1.79450551\n",
      "Iteration 4, loss = 1.67974020\n",
      "Iteration 5, loss = 1.52881570\n",
      "Iteration 6, loss = 1.35088668\n",
      "Iteration 7, loss = 1.16670612\n",
      "Iteration 8, loss = 0.99724847\n",
      "Iteration 9, loss = 0.85256788\n",
      "Iteration 10, loss = 0.73324030\n",
      "Iteration 11, loss = 0.63494955\n",
      "Iteration 12, loss = 0.55431074\n",
      "Iteration 13, loss = 0.48650422\n",
      "Iteration 14, loss = 0.42897034\n",
      "Iteration 15, loss = 0.37962112\n",
      "Iteration 16, loss = 0.33760085\n",
      "Iteration 17, loss = 0.30076056\n",
      "Iteration 18, loss = 0.26947188\n",
      "Iteration 19, loss = 0.24195645\n",
      "Iteration 20, loss = 0.21795681\n",
      "Iteration 21, loss = 0.19729265\n",
      "Iteration 22, loss = 0.17911875\n",
      "Iteration 23, loss = 0.16328538\n",
      "Iteration 24, loss = 0.14948283\n",
      "Iteration 25, loss = 0.13712468\n",
      "Iteration 26, loss = 0.12638803\n",
      "Iteration 27, loss = 0.11675095\n",
      "Iteration 28, loss = 0.10826557\n",
      "Iteration 29, loss = 0.10069152\n",
      "Iteration 30, loss = 0.09397212\n",
      "Iteration 31, loss = 0.08781606\n",
      "Iteration 32, loss = 0.08229619\n",
      "Iteration 33, loss = 0.07740927\n",
      "Iteration 34, loss = 0.07299356\n",
      "Training loss did not improve more than tol=0.015000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.99029385\n",
      "Iteration 2, loss = 1.87946529\n",
      "Iteration 3, loss = 1.79451503\n",
      "Iteration 4, loss = 1.67962953\n",
      "Iteration 5, loss = 1.52868484\n",
      "Iteration 6, loss = 1.35066223\n",
      "Iteration 7, loss = 1.16627422\n",
      "Iteration 8, loss = 0.99656138\n",
      "Iteration 9, loss = 0.85162508\n",
      "Iteration 10, loss = 0.73207929\n",
      "Iteration 11, loss = 0.63362208\n",
      "Iteration 12, loss = 0.55271780\n",
      "Iteration 13, loss = 0.48497615\n",
      "Iteration 14, loss = 0.42738305\n",
      "Iteration 15, loss = 0.37796930\n",
      "Iteration 16, loss = 0.33586372\n",
      "Iteration 17, loss = 0.29905558\n",
      "Iteration 18, loss = 0.26771157\n",
      "Iteration 19, loss = 0.24031253\n",
      "Iteration 20, loss = 0.21636971\n",
      "Iteration 21, loss = 0.19583134\n",
      "Iteration 22, loss = 0.17757348\n",
      "Iteration 23, loss = 0.16176173\n",
      "Iteration 24, loss = 0.14819346\n",
      "Iteration 25, loss = 0.13591014\n",
      "Iteration 26, loss = 0.12505753\n",
      "Iteration 27, loss = 0.11553719\n",
      "Iteration 28, loss = 0.10712416\n",
      "Iteration 29, loss = 0.09955434\n",
      "Iteration 30, loss = 0.09285549\n",
      "Iteration 31, loss = 0.08680553\n",
      "Iteration 32, loss = 0.08134551\n",
      "Iteration 33, loss = 0.07651417\n",
      "Iteration 34, loss = 0.07217298\n",
      "Training loss did not improve more than tol=0.015000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98952497\n",
      "Iteration 2, loss = 1.87981844\n",
      "Iteration 3, loss = 1.79541230\n",
      "Iteration 4, loss = 1.68108120\n",
      "Iteration 5, loss = 1.53145724\n",
      "Iteration 6, loss = 1.35489279\n",
      "Iteration 7, loss = 1.17211190\n",
      "Iteration 8, loss = 1.00321146\n",
      "Iteration 9, loss = 0.85752457\n",
      "Iteration 10, loss = 0.73714062\n",
      "Iteration 11, loss = 0.63889223\n",
      "Iteration 12, loss = 0.55683883\n",
      "Iteration 13, loss = 0.48869306\n",
      "Iteration 14, loss = 0.43078610\n",
      "Iteration 15, loss = 0.38101049\n",
      "Iteration 16, loss = 0.33850463\n",
      "Iteration 17, loss = 0.30180252\n",
      "Iteration 18, loss = 0.27010069\n",
      "Iteration 19, loss = 0.24263915\n",
      "Iteration 20, loss = 0.21898452\n",
      "Iteration 21, loss = 0.19831740\n",
      "Iteration 22, loss = 0.18011413\n",
      "Iteration 23, loss = 0.16429623\n",
      "Iteration 24, loss = 0.15041229\n",
      "Iteration 25, loss = 0.13818628\n",
      "Iteration 26, loss = 0.12738525\n",
      "Iteration 27, loss = 0.11773988\n",
      "Iteration 28, loss = 0.10920837\n",
      "Iteration 29, loss = 0.10159125\n",
      "Iteration 30, loss = 0.09477178\n",
      "Iteration 31, loss = 0.08875189\n",
      "Iteration 32, loss = 0.08326375\n",
      "Iteration 33, loss = 0.07827314\n",
      "Iteration 34, loss = 0.07387132\n",
      "Training loss did not improve more than tol=0.015000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98961587\n",
      "Iteration 2, loss = 1.88000376\n",
      "Iteration 3, loss = 1.79560644\n",
      "Iteration 4, loss = 1.68120748\n",
      "Iteration 5, loss = 1.53152054\n",
      "Iteration 6, loss = 1.35492640\n",
      "Iteration 7, loss = 1.17238880\n",
      "Iteration 8, loss = 1.00418004\n",
      "Iteration 9, loss = 0.85916888\n",
      "Iteration 10, loss = 0.73939210\n",
      "Iteration 11, loss = 0.64142788\n",
      "Iteration 12, loss = 0.55960917\n",
      "Iteration 13, loss = 0.49163765\n",
      "Iteration 14, loss = 0.43368187\n",
      "Iteration 15, loss = 0.38383461\n",
      "Iteration 16, loss = 0.34116799\n",
      "Iteration 17, loss = 0.30429800\n",
      "Iteration 18, loss = 0.27229482\n",
      "Iteration 19, loss = 0.24465867\n",
      "Iteration 20, loss = 0.22074050\n",
      "Iteration 21, loss = 0.19982851\n",
      "Iteration 22, loss = 0.18149246\n",
      "Iteration 23, loss = 0.16546746\n",
      "Iteration 24, loss = 0.15144093\n",
      "Iteration 25, loss = 0.13911664\n",
      "Iteration 26, loss = 0.12814221\n",
      "Iteration 27, loss = 0.11837424\n",
      "Iteration 28, loss = 0.10975611\n",
      "Iteration 29, loss = 0.10207441\n",
      "Iteration 30, loss = 0.09519623\n",
      "Iteration 31, loss = 0.08908637\n",
      "Iteration 32, loss = 0.08357947\n",
      "Iteration 33, loss = 0.07857029\n",
      "Iteration 34, loss = 0.07412680\n",
      "Training loss did not improve more than tol=0.015000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98953393\n",
      "Iteration 2, loss = 1.87976576\n",
      "Iteration 3, loss = 1.79506585\n",
      "Iteration 4, loss = 1.68066331\n",
      "Iteration 5, loss = 1.53072963\n",
      "Iteration 6, loss = 1.35368246\n",
      "Iteration 7, loss = 1.17087880\n",
      "Iteration 8, loss = 1.00241207\n",
      "Iteration 9, loss = 0.85735202\n",
      "Iteration 10, loss = 0.73776871\n",
      "Iteration 11, loss = 0.63984191\n",
      "Iteration 12, loss = 0.55825306\n",
      "Iteration 13, loss = 0.49047145\n",
      "Iteration 14, loss = 0.43291479\n",
      "Iteration 15, loss = 0.38330940\n",
      "Iteration 16, loss = 0.34079739\n",
      "Iteration 17, loss = 0.30407652\n",
      "Iteration 18, loss = 0.27223006\n",
      "Iteration 19, loss = 0.24466863\n",
      "Iteration 20, loss = 0.22074555\n",
      "Iteration 21, loss = 0.20000250\n",
      "Iteration 22, loss = 0.18157961\n",
      "Iteration 23, loss = 0.16558200\n",
      "Iteration 24, loss = 0.15157081\n",
      "Iteration 25, loss = 0.13924432\n",
      "Iteration 26, loss = 0.12825855\n",
      "Iteration 27, loss = 0.11858979\n",
      "Iteration 28, loss = 0.10989206\n",
      "Iteration 29, loss = 0.10221132\n",
      "Iteration 30, loss = 0.09534090\n",
      "Iteration 31, loss = 0.08922477\n",
      "Iteration 32, loss = 0.08372958\n",
      "Iteration 33, loss = 0.07867017\n",
      "Iteration 34, loss = 0.07420843\n",
      "Training loss did not improve more than tol=0.015000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98931278\n",
      "Iteration 2, loss = 1.87942673\n",
      "Iteration 3, loss = 1.79475271\n",
      "Iteration 4, loss = 1.68043306\n",
      "Iteration 5, loss = 1.53078244\n",
      "Iteration 6, loss = 1.35378825\n",
      "Iteration 7, loss = 1.17149609\n",
      "Iteration 8, loss = 1.00298087\n",
      "Iteration 9, loss = 0.85751561\n",
      "Iteration 10, loss = 0.73767548\n",
      "Iteration 11, loss = 0.63956001\n",
      "Iteration 12, loss = 0.55777210\n",
      "Iteration 13, loss = 0.48957965\n",
      "Iteration 14, loss = 0.43184343\n",
      "Iteration 15, loss = 0.38204240\n",
      "Iteration 16, loss = 0.33938195\n",
      "Iteration 17, loss = 0.30257204\n",
      "Iteration 18, loss = 0.27067023\n",
      "Iteration 19, loss = 0.24314720\n",
      "Iteration 20, loss = 0.21917586\n",
      "Iteration 21, loss = 0.19851338\n",
      "Iteration 22, loss = 0.18018058\n",
      "Iteration 23, loss = 0.16423912\n",
      "Iteration 24, loss = 0.15024055\n",
      "Iteration 25, loss = 0.13800193\n",
      "Iteration 26, loss = 0.12713523\n",
      "Iteration 27, loss = 0.11764576\n",
      "Iteration 28, loss = 0.10906041\n",
      "Iteration 29, loss = 0.10138373\n",
      "Iteration 30, loss = 0.09457443\n",
      "Iteration 31, loss = 0.08846993\n",
      "Iteration 32, loss = 0.08303201\n",
      "Iteration 33, loss = 0.07801968\n",
      "Iteration 34, loss = 0.07364508\n",
      "Training loss did not improve more than tol=0.015000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98926949\n",
      "Iteration 2, loss = 1.87940084\n",
      "Iteration 3, loss = 1.79469698\n",
      "Iteration 4, loss = 1.68041457\n",
      "Iteration 5, loss = 1.53073868\n",
      "Iteration 6, loss = 1.35356041\n",
      "Iteration 7, loss = 1.17131804\n",
      "Iteration 8, loss = 1.00256128\n",
      "Iteration 9, loss = 0.85709366\n",
      "Iteration 10, loss = 0.73729533\n",
      "Iteration 11, loss = 0.63917358\n",
      "Iteration 12, loss = 0.55741713\n",
      "Iteration 13, loss = 0.48928307\n",
      "Iteration 14, loss = 0.43149909\n",
      "Iteration 15, loss = 0.38167070\n",
      "Iteration 16, loss = 0.33907478\n",
      "Iteration 17, loss = 0.30232668\n",
      "Iteration 18, loss = 0.27044938\n",
      "Iteration 19, loss = 0.24299507\n",
      "Iteration 20, loss = 0.21913031\n",
      "Iteration 21, loss = 0.19857489\n",
      "Iteration 22, loss = 0.18028799\n",
      "Iteration 23, loss = 0.16440370\n",
      "Iteration 24, loss = 0.15041565\n",
      "Iteration 25, loss = 0.13826786\n",
      "Iteration 26, loss = 0.12730693\n",
      "Iteration 27, loss = 0.11782413\n",
      "Iteration 28, loss = 0.10922872\n",
      "Iteration 29, loss = 0.10157179\n",
      "Iteration 30, loss = 0.09474383\n",
      "Iteration 31, loss = 0.08864662\n",
      "Iteration 32, loss = 0.08318145\n",
      "Iteration 33, loss = 0.07818371\n",
      "Iteration 34, loss = 0.07380243\n",
      "Training loss did not improve more than tol=0.015000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.98934365\n",
      "Iteration 2, loss = 1.87963830\n",
      "Iteration 3, loss = 1.79520231\n",
      "Iteration 4, loss = 1.68137824\n"
     ]
    }
   ],
   "source": [
    "Test_pred = [main(model, 'train.csv', 'test.csv', 10) for model in [MLP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "6yAfnngPiWBg"
   },
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "id = np.arange(len(Test_pred[0]))\n",
    "Test_pred = np.vstack([id, np.array(Test_pred)]).T\n",
    "save = pd.DataFrame(Test_pred) \n",
    "save.to_csv(\"Test_pred.csv\", header = ['id', 'subreddit'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne6wxxy-uHUn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Mini Project 2_different models",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
